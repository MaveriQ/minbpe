{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee1f273-924c-4c73-b64f-d17ebcd2a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import regex as re\n",
    "import pdb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8efb96-142e-4c74-b47d-725e559fb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token_id=0\n",
    "unk_tokens = [112, 120] # excluded from coverage due to low frequency during training. replaced with unk_tokens\n",
    "vocab = [32, 97, 101, 108, 109, 112, 115, 0] # 116 is not in vocabulary i.e not seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed6acc7-bb3d-4dc0-b12b-36b876698db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_SPECIAL_TOKENS = {\n",
    "    '<|unk|>': 0,\n",
    "    '<s>': 1,\n",
    "    '</s>': 2,\n",
    "    '<|pad|>': 3,\n",
    "}\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT2_SPLIT_PATTERN_SINGLE = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}{1}| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "LLAMA_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\" # gpt4 with digits split\n",
    "\n",
    "num_special_tokens = len(LLAMA_SPECIAL_TOKENS)\n",
    "compiled_pattern = re.compile(LLAMA_SPLIT_PATTERN)\n",
    "character_coverage=0.9995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507e7047-dd2e-46d8-a214-6164ab97f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is sample text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225ecb81-57bd-4fa1-ab56-c4d6fa75fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "codepoints = [ord(x) for x in text] # unicode encoding\n",
    "# codepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cfec3ba-cac1-41b4-880a-3f8a71c9aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "codepoints = [x if not x in unk_tokens else unk_token_id for x in codepoints] # replacing unk_tokens with unk_token_id\n",
    "# codepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a13096-fe1c-483d-9a71-dd9e5aa6b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x if x in vocab else chr(x).encode('utf-8') for x in codepoints] # finding utf-8 encoding of codepoints not seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9d399a-e61c-41ed-bc54-e469173ae2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "codepoints = [x if x in vocab else int.from_bytes(chr(x).encode('utf-8')) for x in codepoints] # convert utf-8 bytes to ints\n",
    "# codepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb2fc48-b955-47d8-b265-9629f4f693ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text,character_coverage=0.9995):\n",
    "    \n",
    "    # Build rare_tokens from coverage\n",
    "    vocab_counts = Counter(text)\n",
    "    freq_sort = sorted(vocab_counts.items(), key=lambda item: (-item[1], item[0])) # sort by freq (item[1]) and break ties alphabetically (item[0]).\n",
    "    freq_sort = OrderedDict(freq_sort)\n",
    "        \n",
    "    total_chars = len(text)\n",
    "    coverage_count = int((1-character_coverage) * total_chars)\n",
    "    rare_tokens = [char for char,freq in freq_sort.items() if freq<coverage_count]\n",
    "    unicode_vocab = [char for char,freq in freq_sort.items() if freq>=coverage_count]\n",
    "\n",
    "    num_special_tokens = len(LLAMA_SPECIAL_TOKENS)\n",
    "    vocab_itos = {v:k for k,v in LLAMA_SPECIAL_TOKENS.items()} # add special tokens\n",
    "    vocab_itos.update({idx+num_special_tokens: bytes([idx]) for idx in range(256)}) # add utf-8 fallback bytes\n",
    "    vocab_itos.update({idx+num_special_tokens + 256: ch for idx,ch in enumerate(unicode_vocab)}) # add utf-8 fallback bytes\n",
    "    return vocab_itos, rare_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db987159-3695-42c0-8772-d3099bbb071a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_itos, rare_tokens = build_vocab(text)\n",
    "vocab_stoi = {v:k for k,v in vocab_itos.items()}\n",
    "# vocab_itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8d6cea-9142-4892-986e-56ccb7f1e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(text,vocab_itos=vocab_itos,rare_tokens=rare_tokens,verbose=False):\n",
    "    vocab_stoi = {v:k for k,v in vocab_itos.items()}\n",
    "    unk_token_id = 0\n",
    "    token_ids = []\n",
    "    for x in text:\n",
    "        if x in rare_tokens:\n",
    "            token_ids.append(unk_token_id)\n",
    "        elif x in vocab_stoi:\n",
    "            token_ids.append(vocab_stoi[x])\n",
    "        else:\n",
    "            if verbose: print(f'utf-8 encoded : {x}')\n",
    "            x_utf8 = x.encode(\"utf-8\")\n",
    "            x_utf8_token_ids = [i+4 for i in x_utf8] #TODO Fix the hack of adding special tokens manually\n",
    "            token_ids.extend(x_utf8_token_ids)\n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c12ed-5e23-45e2-ba7e-1dfa211ee98b",
   "metadata": {},
   "source": [
    "# Code from Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11bc41ea-1745-4116-ad53-bb9341b8542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_llama(ids,counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, updates a dictionary of counts of consecutive pairs (called stats), skipping over unk_token_id\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        if unk_token_id in pair: # ignore pairs with unk_tokens\n",
    "            continue\n",
    "        # if type(vocab_itos[pair[0]])!=type(vocab_itos[pair[1]]): # don't merge across utf-8 and unicode encoding\n",
    "        #     continue\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge_llama(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        \n",
    "        # if unk_token_id, skip over it\n",
    "        if ids[i] == unk_token_id:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        elif ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7598bf5b-2c56-4815-9005-c4c58e1b1da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186548"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\\n\".join(open('tests/taylorswift.txt').readlines())\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4b902fb-f3d2-4d48-9d71-6288a1700061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(text, required_vocab_size=350, verbose=False, character_coverage=0.9995):\n",
    "# pdb.set_trace()\n",
    "    # required_vocab_size = 350\n",
    "    vocab_itos, rare_tokens = build_vocab(text,character_coverage=character_coverage)\n",
    "    vocab_len = len(vocab_itos)\n",
    "    assert required_vocab_size >= vocab_len, f\"{required_vocab_size} < {vocab_len}\"\n",
    "    num_merges = required_vocab_size - vocab_len\n",
    "    if verbose: print(f\"New Merges : {num_merges}\")\n",
    "    \n",
    "    # split the text up into text chunks\n",
    "    text_chunks = re.findall(compiled_pattern, text)\n",
    "    \n",
    "    # Get Unicode codepoints for each chunk. Replace the characters in unk_tokens with unk_token_id.\n",
    "    ids = [get_token_ids(chunk,vocab_itos=vocab_itos) for chunk in text_chunks] # unicode encoded\n",
    "    \n",
    "    # iteratively merge the most common pairs to create new tokens\n",
    "    merges = {} # (int, int) -> int\n",
    "    # vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "    \n",
    "    for i in tqdm(range(num_merges),total=num_merges,disable=verbose):\n",
    "        # count the number of times every consecutive pair appears\n",
    "        counts = {}\n",
    "        for chunk_ids in ids:\n",
    "            # passing in counts will update it in place, adding up counts\n",
    "            get_counts_llama(chunk_ids,counts)\n",
    "        # find the pair with the highest count\n",
    "        # pdb.set_trace()\n",
    "        pair = max(counts, key=counts.get)\n",
    "        # if any(x in range(4,256) for x in pair):\n",
    "        #     pdb.set_trace()\n",
    "        # mint a new token: assign it the next available id\n",
    "        idx = vocab_len + i\n",
    "        # replace all occurrences of pair in ids with idx\n",
    "        ids = [merge_llama(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "        # save the merge\n",
    "        merges[pair] = idx\n",
    "        vocab_itos[idx] = vocab_itos[pair[0]] + vocab_itos[pair[1]] # if isinstance(new_merge,str) else new_merge.decode('utf-8')\n",
    "        # prints\n",
    "        if verbose:\n",
    "            print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab_itos[idx]}) had {counts[pair]} occurrences\")\n",
    "\n",
    "    # vocab.update({idx+num_special_tokens+256+num_merges: char for idx,char in enumerate(unicode_vocab)}) # add unicode characters\n",
    "    return vocab_itos,merges,rare_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58b0feb-b125-47d6-9afd-d55d6759fc33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:02<00:00, 17.52it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_itos,merges,rare_tokens=train(text,required_vocab_size=400,character_coverage=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7993bf05-bc01-4b30-b9c7-20a7f9dd4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text,verbose=False):\n",
    "      # given a string, return list of integers (the tokens)\n",
    "    tokens = get_token_ids(text,vocab_itos=vocab_itos,rare_tokens=rare_tokens,verbose=verbose)\n",
    "    while True:\n",
    "        counts = get_counts_llama(tokens)\n",
    "        # pdb.set_trace()\n",
    "        pair = min(counts, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            if verbose: print(f\"breaking out on pair : {pair}\")\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge_llama(tokens, pair, idx)\n",
    "        if len(tokens) < 2 or (len(tokens)==2 and unk_token_id in tokens):\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2af74d9-ca3c-4f8f-9999-a21d2bfa6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"My name is Taylor Swift and this is ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eeb7f52-9ca1-4ee5-a8ac-ed2f8c8aaaae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc = encode(test,verbose=False)\n",
    "# enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d53a7756-31ea-4db7-b0c8-5afd01b72eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    text=\"\"\n",
    "    utf8_chunk=[]\n",
    "    uni_chunk=[]\n",
    "    for id in ids:\n",
    "        if id==0:\n",
    "            text += \"<|unk|>\"\n",
    "            continue\n",
    "        if id<260:\n",
    "            if len(uni_chunk)>0:\n",
    "                text += \"\".join(vocab_itos[idx] for idx in uni_chunk)\n",
    "                uni_chunk=[]\n",
    "            utf8_chunk.append(id)\n",
    "        else:\n",
    "            if len(utf8_chunk)>0:\n",
    "                tokens = b\"\".join(vocab_itos[idx] for idx in utf8_chunk)\n",
    "                text += tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "                utf8_chunk=[]\n",
    "            uni_chunk.append(id)\n",
    "    #flush out the last chunk\n",
    "    if len(utf8_chunk)>0:\n",
    "        tokens = b\"\".join(vocab_itos[idx] for idx in utf8_chunk)\n",
    "        text += tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    else:\n",
    "        text += \"\".join(vocab_itos[idx] for idx in uni_chunk)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b56fa1c-0e85-4810-b7d4-88c129229dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vocab_itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95afe1b0-c66b-46f1-89f5-8fca72cba07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Taylor Swift and this is ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "print(decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c53ddaba-09bf-41bc-8786-ff7305a257c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Taylor Swift and this is ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„\n",
      "My name is Taylor Swift and this is ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(decode(encode(test))),print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02dda0b3-2697-4a13-8642-aef85b24a45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(test))==test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "867a739f-0945-4a14-baa8-aaf414b9513c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in test.split('\\n'):\n",
    "    if decode(encode(line))!=line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f86c3b0d-8f0e-46ff-ba90-1f653dcf3968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"\\n\".join(open('toy.txt').readlines())\n",
    "decode(encode(test))==test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f5ebd-2eb7-4522-a2ab-4d72a0781dd2",
   "metadata": {},
   "source": [
    "# Handling UNK tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6ce42fd-e070-4202-a708-a2dc878fac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:04<00:00, 14.86it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_itos,merges,rare_tokens=train(text,required_vocab_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa4dff30-788b-4ef1-9c39-518f4ef12e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[';', 'j', 'â€“', '$', 'q', '&', '?', 'Z', 'â€”', '!', 'â€¢', '/', 'Q', 'X', '\\t', 'Ã©', '#', '@', '+', 'Â£', 'Â®', 'Ã¡', 'Ã­', 'Ã±', 'Ã¶', 'â„¢']\n"
     ]
    }
   ],
   "source": [
    "print(rare_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2168d166-d053-4e4e-bfad-d7594e49afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens+='Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "057858e8-e972-48d4-8b06-02909c0ac7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 361, 264, 289, 272, 264, 285, 263, 266]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('Xenophobia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "908ece32-2909-41f7-927e-32dd111c072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>ou'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode('You'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "450d6092-2aea-43df-8e2a-fecac686c9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 264, 275]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_token_ids('You',vocab_itos=vocab_itos,rare_tokens=rare_tokens,verbose=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a60a2141-546d-4db4-90dc-f73ad156c187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(264, 275): 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = get_counts_llama(tokens)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4e00625-f3a0-43d2-9196-f5b73ed2753b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 275)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = min(counts, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2381b289-8f3c-4f43-8a64-e772bde50f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "[0, 380]\n"
     ]
    }
   ],
   "source": [
    "idx = merges[pair]\n",
    "print(idx)\n",
    "tokens = merge_llama(tokens, pair, idx)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e6406aa-872c-4e4b-a3bc-1669dd66457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40fc2507-e879-4144-8af9-3ef6fa3da4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8258"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('â‚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7be5e4-01a6-4d30-b301-cc246a7d0eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
